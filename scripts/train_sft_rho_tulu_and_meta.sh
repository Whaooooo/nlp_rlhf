python -m realhf.apps.quickstart mr-sft \
    experiment_name=rho-tulu-sft \
    trial_name=lr0.00003-test1 \
    allocation_mode=manual \
    mode=local \
    n_nodes=1 \
    n_gpus_per_node=1 \
    exp_ctrl.total_train_epochs=1 \
    exp_ctrl.save_freq_epochs=1 \
    exp_ctrl.eval_freq_epochs=1 \
    model.type._class=llama \
    model.backend=megatron \
    model.path=/root/autodl-tmp/base_model/models--microsoft--rho-math-1b-v0.1/snapshots/0b1db8d22b330c281cc810899d7938f023d78195 \
    model.optimizer.lr=0.00003 \
    model.optimizer.weight_decay=0.0 \
    model.optimizer.warmup_steps_proportion=0.1 \
    dataset.train_path=/root/autodl-tmp/dataset/sft/chat/tulu-v2-sft-mixture/tulu-v2-sft-mixture_train.json \
    dataset.valid_path=/root/autodl-tmp/dataset/sft/chat/tulu-v2-sft-mixture/tulu-v2-sft-mixture_test.json \
    dataset.max_seqlen=2048 \
    allocation.n_mbs=16 \
    allocation.parallel.pipeline_parallel_size=1 \
    allocation.parallel.model_parallel_size=1 \
    allocation.parallel.data_parallel_size=1 \
    dataset.train_bs_n_seqs=1024 \
    dataset.valid_bs_n_seqs=128
